{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot Python NLTK ROBELE - Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM7MW9YGQLv0FdGbeO3pVJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudaruda/Chatbots/blob/main/ChatBot_Python_NLTK_ROBELE_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98RbeCvxoqKo"
      },
      "source": [
        "## Bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY26VSVsVhaK"
      },
      "source": [
        "import io, random, string, warnings\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw9Y2ivAobAA"
      },
      "source": [
        "### Instalando Pacotes do NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxM1op9WobTc",
        "outputId": "53e7350b-5cd1-4275-9b12-2099de8de442"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk.download() # for downloading packages\n",
        "nltk.download('popular', quiet=True) # for downloading packages\n",
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-etoVV_jtvDB"
      },
      "source": [
        "### Intalando Pcotes de Stem (em Portugues)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRDVI6pEtvRO"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "englishStemmer=SnowballStemmer(\"portuguese\")\n",
        "#englishStemmer.stem(\"continha\")"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soi-Psy5cdTm"
      },
      "source": [
        "## Carregando o conhecimento do CHATBOT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-X89bhTVlAM"
      },
      "source": [
        "f=open('chatbot2.txt','r',errors = 'ignore')\n",
        "raw=f.read()\n",
        "raw=raw.lower()# converts to lowercase"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YBBWMjJcWsC"
      },
      "source": [
        "## Tockenização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04ynx0skVyuN",
        "outputId": "20b0e8c7-2c1f-46ff-9082-9e40eccd823a"
      },
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)# converte List->Sentença \n",
        "word_tokens = nltk.word_tokenize(raw)# converte List->Palavra\n",
        "stopwords_ok = set(stopwords.words('portuguese') + list(punctuation)) # coleção de stopwords\n",
        "print('Exemplo \"sent_tokens\":\\n',sent_tokens[:2])\n",
        "print('Exemplo \"word_tokens\":\\n',word_tokens[:5])\n",
        "print('Exemplo \"stopwords_ok\"\\n',stopwords_ok)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exemplo \"sent_tokens\":\n",
            " ['chatbot (ou chatterbot) é um programa de computador que tenta simular um ser humano na conversação com as pessoas.', '[1] o objetivo é responder as perguntas de tal forma que as pessoas tenham a impressão de estar conversando com outra pessoa e não com um programa de computador.']\n",
            "Exemplo \"word_tokens\":\n",
            " ['chatbot', '(', 'ou', 'chatterbot', ')']\n",
            "Exemplo \"stopwords_ok\"\n",
            " {'na', 'formos', 'terei', 'já', 'meu', 'teus', 'estavam', 'houveriam', 'fosse', 'estão', 'estivermos', 'das', 'essas', 'sejamos', 'deles', 'tenham', 'houverem', 'dela', '+', 'lhes', 'ela', 'suas', 'hei', 'seu', 'teve', 'fui', 'tivermos', 'isso', 'tivessem', '!', 'aquelas', '[', 'somos', 'sejam', 'nossos', 'aquilo', 'como', 'sua', 'um', 'lhe', 'por', 'é', 'os', 'às', '$', 'da', 'te', 'mas', 'este', 'meus', ')', 'hajam', 'à', '-', '&', 'nem', 'estejamos', 'tivéramos', 'teria', 'vocês', 'também', 'sou', 'estamos', 'estive', 'fomos', '~', 'se', '<', 'no', 'estejam', 'estiverem', 'for', 'nos', 'hajamos', 'teríamos', 'nas', 'só', 'sem', 'seríamos', 'de', 'foram', 'mesmo', 'estivera', 'seja', 'tínhamos', 'houvermos', 'houverá', 'fôramos', ',', 'nosso', 'houverei', 'tinha', '|', 'está', 'tuas', 'seremos', '^', 'estivéramos', 'o', 'nossa', 'houvessem', 'em', 'terão', 'vos', 'fôssemos', 'foi', '@', 'tiveram', 'esses', 'nossas', 'ao', 'teremos', '%', 'estou', 'estava', 'tua', 'aqueles', 'para', 'muito', 'esteja', 'até', '{', 'tivera', 'éramos', 'minhas', 'qual', 'houvéramos', ':', 'houvemos', 'essa', 'pelo', 'tinham', '?', 'tivesse', 'eram', '/', '}', '#', 'eu', 'esse', 'tivemos', 'e', 'num', 'mais', '_', '`', 'estiveram', 'há', 'fora', 'tivéssemos', 'depois', 'tenha', 'seus', ';', '\\\\', '\"', 'teu', 'minha', 'a', 'houver', 'tiver', 'aquele', 'tenhamos', 'havemos', '>', 'não', 'seria', 'elas', 'esta', 'haja', 'houverão', 'seriam', 'houveremos', 'tu', '.', 'tem', 'estávamos', 'isto', '(', 'delas', 'estes', 'estivesse', 'houvera', 'forem', 'estivéssemos', 'são', 'temos', 'houveria', 'era', 'tém', 'houvéssemos', 'pelos', 'será', 'eles', 'quem', 'estas', 'com', 'estivessem', 'serão', 'serei', 'terá', 'esteve', 'estivemos', 'uma', '=', 'do', 'dos', 'entre', 'houve', 'ou', 'nós', 'dele', 'houveram', 'fossem', 'pela', 'teriam', 'tive', 'hão', 'tiverem', 'aos', ']', 'houvesse', 'pelas', 'estiver', 'tenho', 'você', 'me', 'as', 'numa', 'quando', \"'\", 'aquela', '*', 'ele', 'que', 'houveríamos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUwlhwOycOcX"
      },
      "source": [
        "## Pre-processamento\n",
        "WordNet é um dicionario semantico em Inglês(pode ser um ponto fraco)<br>\n",
        "Por isso, vamos tentar com altertiva de Stemming em portugues porque existem variações de linguistica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFk4ABoXYBOb"
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "lemmer2 = englishStemmer\n",
        "def LemTokens(tokens):\n",
        "    #return [lemmer.lemmatize(token) for token in tokens]\n",
        "    return [lemmer2.stem(token) for token in tokens] #ALTERNATIVA AQUI\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIY21MjMxa7R"
      },
      "source": [
        "## Encontro de palavras (Match Keywords)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTlQlE7JxbSu"
      },
      "source": [
        "GREETING_INPUTS = (\"olá\",\"como vai você\",\"como vai\",\"e ai\",\"blz\",\"bele\",\"beleza\",\"bom dia\",\"boa tarde\",\"boa noite\",\"dia\",\"tarde\",\"noite\",\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"aew\", \"o que manda hj?\", \"...hummm...\", \"to aqui!\", \"oi\", \"To di boa! Agora q vc chegou :-)\"]\n",
        "\n",
        "# Checking for greetings\n",
        "def greeting(sentence):\n",
        "    \"\"\"Pergunta entrada respondida imediatamente\"\"\"\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FhNxwP2yL8R"
      },
      "source": [
        "## Gerando a resposta (Function Generating Response)\n",
        "\n",
        "### Grupos de palavras (Bag of Words)\n",
        "Depois do processamento da frase, nos precisamos transforma Texto em Vetor de Números.\n",
        "Os grupos de palavras são a representação de textos que estão descritos as ocorrencias que estão contidas no docjmento. Cumprindo 2 requisitos:\n",
        "- Um vocabulario de palavras conhecidas.\n",
        "- Avaliação de palavras conhecidas.\n",
        "\n",
        "A \"inteligencia\" por trás são esses \"Grupos de Palavras\"/\"Bag of Words\". Com esse metodo é possivel obter algum aprendizado sobre conteúdo do documento.\n",
        "\n",
        "Abrodagem TF-IDF\n",
        "É preciso \"tomar cuidado\" com palavras muito frequentes que podem ter menos conteúdo.\n",
        "Outro problema deste método é ele da mais importancia aos paragrafos mais longos.\n",
        "\n",
        "Uma solução para esse problema é re-escalr a a frequencia das palavras que aparecem em todos os paragrafos com uma penalização. O nome desta abordagem é \"Term Frequency-Inverse Document Frequency\", ou \"TF-IDF for short\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jj9bN7c-yMQb"
      },
      "source": [
        "# Generating response\n",
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=stopwords_ok)\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"Como eh? humm?\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD_VJnq42drv"
      },
      "source": [
        "# Finalmente\n",
        "Vamos poder ver o rôbo em ação:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwe8mMXbYLaF",
        "outputId": "24a8421d-122f-4162-a87b-f6cc791a1bef"
      },
      "source": [
        "flag=True\n",
        "print(\"ROBELE: Meu nome é ROBELE. Eu respondo coisas sobre Chatbots. Quando quiser vaza, digita Xau!\")\n",
        "\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(not user_response in ['bye','xau']):\n",
        "        if(user_response in ['vlw','obrigado'] or user_response=='flww' ):\n",
        "            flag=False\n",
        "            print(\"ROBELE: e aew...\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBELE: \"+greeting(user_response))\n",
        "            else:\n",
        "                sent_tokens.append(user_response)\n",
        "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "                final_words=list(set(word_tokens))\n",
        "                print(\"ROBELE: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBELE: Xau! Pa tu tb\")    "
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROBELE: Meu nome é ROBELE. Eu respondo coisas sobre Chatbots. Quando quiser vaza, digita Xau!\n",
            "quando foi criado os chatbots\n",
            "ROBELE: os chatbots alice e bob criados pelo facebook estavam criando um idioma próprio e passaram a agir de forma diferente da que foi descrita por seus programadores no início do projeto.\n",
            "o que é chatbot\n",
            "ROBELE: chatbot (ou chatterbot) é um programa de computador que tenta simular um ser humano na conversação com as pessoas.\n",
            "como crio um chatbot\n",
            "ROBELE: os chatbots alice e bob criados pelo facebook estavam criando um idioma próprio e passaram a agir de forma diferente da que foi descrita por seus programadores no início do projeto.\n",
            "como faço chatbot\n",
            "ROBELE: chatbot (ou chatterbot) é um programa de computador que tenta simular um ser humano na conversação com as pessoas.\n",
            "como é construido um chat\n",
            "ROBELE: [6]\n",
            "\n",
            "para criar chatbots é necessário uma plataforma de sistema computacional, geralmente fornecidos no modelo de computação na nuvem (cloud computing) e surgiram para auxiliar pessoas[7] sem ter a necessidade de saber programar ou complexidade de construir uma robusta infraestrutura.\n",
            "aconteceu algo em 2016\n",
            "ROBELE: em agosto de 2016 o whatsapp, maior aplicativo de mensagens do mundo, alterou o seus termos de serviço para incluir chatbots na sua plataforma.\n",
            "tem vantagem o chatbot\n",
            "ROBELE: as empresas que adquirem essa tecnologia, passam a ter uma vantagem competitiva em relação a concorrência, pois conseguem automatizar processos simples de atendimento, de forma reduzir o custo e não deixar o chatbot tão caro.\n",
            "qual a desvatagem\n",
            "ROBELE: Como eh? humm?\n",
            "e a desvantagem\n",
            "ROBELE: tudo tem um lado positivo e um lado negativo, e com a desvantagem do chatbot são as limitações devem ser consideradas antes de sua implementação.\n",
            "a blz\n",
            "ROBELE: ...hummm...\n",
            "blz\n",
            "ROBELE: to aqui!\n",
            "xau\n",
            "ROBELE: Xau! Pa tu tb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVJsEX9K2lcI"
      },
      "source": [
        "# Conclusão\n",
        "- Tecnicamente é uma abordagem muito simples de ser aplicada sobre qualquer tipo de documento de texto que contenha estrutura de paragrafos.\n",
        "- Porém há limitações, dependendo da format como aplicaou a \"Lematização\" pode ser que a sua pergunta não seja compreendida pela aplicação.\n",
        "- Como vimos no teste da aplicação, foi necessário que eu refizesse a mesma pergunta de três formas diferentes \"qual a desvatagem\", \"qual a desvatagem\", \"e a desvantagem\". Esses três termos queriam dizer a mesma coisa, porém com erro de português ou conter termos \"desconhecidos\" no \"Bag of words\" podem deixar a aplicação com grande limitações.\n",
        "- Vejo, já muitas aberturas, para melhorar o algoritmo de \"Lematização\" e evoluir a aplicação a um nível comercialmente aceitavél. Como experiência alcancei o objetivo de construir um ChatBot extremamente simples e poucas horas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbDt4m0cngqW"
      },
      "source": [
        "# Bibliografia\n",
        "\n",
        "- https://github.com/Spandan-Madan/Me_Bot\n",
        "- https://stackoverflow.com/questions/65117858/python-chatbot-using-excel\n",
        "- https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK\n",
        "- https://lars76.github.io/2018/05/08/portuguese-lemmatizers.html\n",
        "- https://stackoverflow.com/questions/57340142/user-warning-your-stop-words-may-be-inconsistent-with-your-preprocessing\n",
        "- https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e\n",
        "- https://www.geeksforgeeks.org/python-stemming-words-with-nltk/"
      ]
    }
  ]
}